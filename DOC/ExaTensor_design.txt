*** ExaTensor: Massively parallel, performance-portable computational
               infrastructure for sparse/dense tensor algebra ***
AUTHOR: Dmitry I. Lyakh (Liakh): quant4me@gmail.com, liakhdi@ornl.gov
REVISION: 2015/09/28 (Project start date: Dec 2013)
Copyright (C) 2015 Dmitry I. Lyakh
Copyright (C) 2015 Oak Ridge National Laboratory (UT-Battelle)
LICENSE: GPL v.2
--------------------------------------------------------------

0. Preambule

Large-scale tensor algebra is an existential prerequisite for
successful simulations in multiple scientific domains, most notably
those based on quantum many-body theory, with applications in quantum
chemistry (electronic structure theory), nuclear physics, and quantum
computing simulations. Yet, to date no single attempt has resulted in
a production-level software flexible enough to be used accross these
computational domains while being performance-portable with respect to
different hardware (CPU, GPU, Xeon Phi, etc.) and with respect to the
size of the HPC system. This was the main motivation to start the
ExaTensor project. In the design of ExaTensor, the author tried to
avoid mistakes and inconsistencies made during the previous efforts
dealing with tensor algebra (mostly within the domain of electronic
structure theory), while leveraging some relevant general-purpose
techniques which were shown to be successful in similar contexts.
On top of that, some novel algorithmic elements are introduced.
Thus, ExaTensor is in some sense a synthetic project based on a
critical analysis of the previous experience, in particular,
that by the author himself.


1. Goals

(a) Hardware-agnostic: Efficiency at the node level, efficient utilization
of diverse hardware: Multicore CPU, NVidia GPU, Intel Xeon Phi, AMD APU, etc.

(b) Scale-agnostic: Efficiency at the HPC level, efficient distributed
data mapping and work scheduling for any given HPC system: From small
clusters to the Leadership HPC systems.

(c) Expertise-agnostic: Ease of use accross different computational domains.


2. Key design elements

(a) Virtual processing: The hardware specifics is hidden behind the
abstraction of virtual processing. Namely, each MPI rank is running
a virtual tensor-algebra processor capable of performing tensor
algebra operations (instructions) on diverse computer architectures.

(b) Hierarchical view of the HPC system combined with the hierarchical
data mapping and data-driven work scheduling: The entire HPC system is
recursively decomposed into smaller units, up to the node level. Each
such a computing unit is running a virtual processor capable of performing
tensor algebra operations at the corresponding level of granularity. As a
consequence, both data and work is recursively split into smaller pieces
and distributed at each level of hierarchy, the work being predominantly
sent close to the participating data objects (tensor arguments).

(c) Compact domain-specific interpreted language: A minimal set of
domain-specific instructions is introduced, yet being capable of
describing basic tensor algebra operations the way they appear
in different scientific domains. In this way, one should be able
to express a tensor algorithm with a minimal amount of special code.
The code can be directly interpreted by the ExaTensor runtime
(virtual processor), where all efficient parallelization decisions
are made by ExaTensor, thus allowing the user to completely focus
on a higher-level expression of the tensor algorithm of interest.


3. ExaTensor software layers

