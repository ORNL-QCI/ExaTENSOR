*** ExaTensor: Massively parallel, performance-portable computational
               infrastructure for sparse/dense tensor algebra ***
AUTHOR: Dmitry I. Lyakh (Liakh): quant4me@gmail.com, liakhdi@ornl.gov
REVISION: 2016/01/21 (Project start date: Dec 2013)

Copyright (C) 2014-2022 Dmitry I. Lyakh (Liakh)
Copyright (C) 2014-2022 Oak Ridge National Laboratory (UT-Battelle)

LICENSE: BSD 3-Clause

-------------------------------------------------------------------

0. Preamble

Large-scale tensor algebra is an existential prerequisite for
successful simulations in multiple scientific domains, most notably
those dealing with quantum many-body theory, with applications in
quantum chemistry (electronic structure theory), nuclear physics, and
quantum computing simulations. Yet, to date no single attempt has resulted
in a production-level software flexible enough to be used accross these
computational domains while being performance-portable with respect to
different hardware (CPU, GPU, Xeon Phi, etc.) and with respect to the
size of the HPC system. This was the main motivation to start the
ExaTensor project. In the design of ExaTensor, the author tried to
avoid mistakes and inconsistencies made during the previous efforts
dealing with tensor algebra (mostly within the domain of electronic
structure theory), while leveraging some relevant general-purpose
techniques which were shown to be successful in similar contexts.
On top of that, some novel algorithmic elements are introduced.
Thus, ExaTensor is in some sense a synthetic project based on a
critical analysis of the previous experience, in particular,
that by the author himself.


1. Goals

(a) Hardware-agnosticism: Efficiency at the node level, efficient utilization
of diverse hardware: Multicore CPU, NVidia GPU, Intel Xeon Phi, AMD GPU/APU, etc.
Efficient utilization of deep memory hierarchy available on node.

(b) Scale-agnosticism: Efficiency at the HPC level, efficient distributed
data mapping and asynchronous task scheduling on any given HPC system:
From small clusters to the Leadership HPC systems.

(c) Expertise-agnostic: Ease of use accross different computational domains.


2. Key design elements

(a) Virtual processing: The hardware specifics is hidden behind the
abstraction of virtual processing. Namely, each MPI rank is running
a virtual tensor-algebra processor capable of performing tensor
algebra operations (instructions) on diverse computer architectures.
The node-level autonomy achieved via the concept of virtual processing
is also beneficial for fault-tolerance.

(b) Hierarchical view of the HPC system combined with the hierarchical
data mapping and data-driven task scheduling: The entire HPC system is
recursively decomposed into smaller units, up to the node level. Each
such a computing unit is running a virtual processor capable of performing
tensor algebra operations at the corresponding level of granularity. As a
consequence, both data and work is recursively split into smaller pieces
and distributed at each level of hierarchy, the work being predominantly
sent to where the participating data objects (tensor arguments) reside.

(c) Compact domain-specific interpreted language: A minimal set of
domain-specific instructions is introduced, yet being capable of
describing basic tensor algebra operations the way they appear
in different scientific domains. In this way, one should be able
to express a tensor algebra algorithm with a minimal amount of
domain-specific code. The code can be directly interpreted by the
ExaTensor runtime (virtual processor), where all efficient
parallelization decisions are made by ExaTensor, thus allowing
the user to completely focus on a higher-level expression of the
tensor algebra algorithm of interest.

(d) Composability: The ExaTensor framework is composed of useful
reusable components which can be deployed autonomously elsewhere.


3. ExaTensor software layers

